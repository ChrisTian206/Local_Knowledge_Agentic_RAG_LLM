{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py pymupdf langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGSMITH_API_KEY'] = 'your langsmith api key'\n",
    "\n",
    "local_llm = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n"
     ]
    }
   ],
   "source": [
    "# Loading multiple PDFs from a source folder\n",
    "from langchain_community.document_loaders import FileSystemBlobLoader\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import PyMuPDFParser\n",
    "\n",
    "loader = GenericLoader(\n",
    "    blob_loader=FileSystemBlobLoader(\n",
    "        path='./data',\n",
    "        glob = '*.pdf',\n",
    "    ),\n",
    "\n",
    "    blob_parser = PyMuPDFParser(),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 500, chunk_overlap = 0,\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current doc splits has metadata in it, we need to remove it\n",
    "#print(doc_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_doc = []\n",
    "\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        if len(doc.page_content) < 7:\n",
    "            continue  # Skip this document if it has less than 7 words\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_doc.append(Document(page_content=doc.page_content, metadata=clean_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(filtered_doc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedInternal: warning: chunking tokenized text at index 0 into zero tokens\n"
     ]
    }
   ],
   "source": [
    "# vectorDB\n",
    "#from langchain_ollama import OllamaEmbeddings, much slower than GPT4AllEmbeddings\n",
    "#from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "#from langchain_community.document_transformers import RankLLMReranker\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = filtered_doc,\n",
    "    collection_name='lk_rag',\n",
    "    embedding= GPT4AllEmbeddings(),\n",
    ")\n",
    "\n",
    "\n",
    "# details: https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html#langchain_chroma.vectorstores.Chroma.as_retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity_score_threshold\",\n",
    "    search_kwargs = {\"k\":5, \"score_threshold\":0.5},\n",
    ")\n",
    "\n",
    "# reranker = RankLLMReranker(\n",
    "#     model_name = \"RankZephyr\",\n",
    "#     top_n = 3,\n",
    "# )\n",
    "\n",
    "# retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor= reranker,\n",
    "#     base_retriever= retriever,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/_wk9p9f500d8xnl6kjnnkbp40000gn/T/ipykernel_35951/248629733.py:7: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_llm, format=\"json\", temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Retrieving similar documents\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0.0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a grader assessing the relevance of a retrieved document to a user question.\n",
    "If the document contains keywords related to the user question, grade it as relevant.\n",
    "This does not need to be a stringent testâ€”the goal is to filter out erroneous retrievals.\n",
    "\n",
    "Provide a binary score as JSON with a single key `\"score\"` and a value of `\"yes\"` or `\"no\"`, without any explanation or extra text.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here is the retrieved document:\n",
    "\n",
    "{documents}\n",
    "\n",
    "Here is the user question:\n",
    "\n",
    "{question}\n",
    "<|eot_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"documents\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output from prompt to llm, then pass to JsonOutputParser(). \"|\" as pipeline chain (LCEL)\n",
    "retrieval_grader = prompt | llm | JsonOutputParser() \n",
    "question  = \"How does MapReduce work?\"\n",
    "\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[0].page_content\n",
    "print(doc_text)\n",
    "#It should print 'yes' if retrievals are relevant; 'no' otherwise.\n",
    "print(retrieval_grader.invoke({\"question\": question, \"documents\": doc_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use a maximum of three sentences and keep the answer concise.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question: {question}\n",
    "Documents: {documents}\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"documents\"]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0.0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serverless computing relies on cloud providers' infrastructure, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). It also depends on the scalability and elasticity of these platforms to handle variable workloads. Additionally, serverless computing is built upon the concept of event-driven programming and function-as-a-service models.\n"
     ]
    }
   ],
   "source": [
    "#run \n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "question = \"what does serverless computing relies on?\"\n",
    "docs = retriever.invoke(question)\n",
    "answer = rag_chain.invoke({\"question\": question, \"documents\": format_docs(docs)})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If No Relevant Info from the Input Local Knowledge Base, Look up Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TAVILY_API_KEY'] = \"tvly-xxxxxx\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check For Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, temperature=0.0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a grader assessing whether an answer is grounded in\n",
    "or supported by a set of facts. Give a binary score 'yes' or 'no'\n",
    "to indicate whether the answer is grounded or supported by the facts.\n",
    "Provide the binary score as a JSON with a single key \"score\" and no preamble or explanation.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here are the facts:\n",
    "\\n --- \\n\n",
    "{documents}\n",
    "\\n --- \\n\n",
    "Here is the answer: {answer}\n",
    "<|eot_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"documents\", \"answer\"]\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, temperature=0.0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a grader assessing whether an answer is useful to answer a question. Give a binary score 'yes' or 'no'\n",
    "to indicate whether the answer is useful to answer a question.\n",
    "Provide the binary score as a JSON with a single key \"score\" and no preamble or explanation.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here are the question:\n",
    "\\n --- \\n\n",
    "{question}\n",
    "\\n --- \\n\n",
    "Here is the answer: {answer}\n",
    "<|eot_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"answer\"]\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "# States\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    State of the graph\n",
    "\n",
    "    Attributes:\n",
    "    - question: The question to be answered\n",
    "    - answer: The answer to the question\n",
    "    - web_search: whether to add web search\n",
    "    - documents: The documents to be used for answering the question\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    answer: str\n",
    "    web_search: bool\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "# Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Retrieve docs from vector store\n",
    "    \n",
    "    input: state(dict):  graph state\n",
    "    output: state(dict): graph state with newly added documents and states.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"*** RETRIEVE ***\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "    return {\"documents\": docs, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generate answer from retrieved documents\n",
    "    \n",
    "    input: state(dict):  graph state\n",
    "    output: state(dict): graph state with newly added answer.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"*** GENERATE ***\")\n",
    "    docs = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "    answer = rag_chain.invoke({\"question\": question, \"documents\": docs})\n",
    "    return {\"documents\": docs, \"answer\": answer, \"question\": question}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Grade the documents\n",
    "    \n",
    "    input: state(dict):  graph state\n",
    "    output: state(dict): graph state with newly added grade.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"*** GRADE DOCUMENTS ***\")\n",
    "    docs = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    web_search = False\n",
    "\n",
    "    for doc in docs:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"documents\": doc.page_content})\n",
    "        grade = score[\"score\"]\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"Graded: Document is relevant\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"Graded: Document is not relevant\")\n",
    "            web_search = True\n",
    "            continue\n",
    "    \n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Search the web\n",
    "    \n",
    "    input: state(dict):  graph state\n",
    "    output: state(dict): graph state with newly added web search results.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"*** WEB SEARCH ***\")\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "\n",
    "    searched_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in searched_docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if docs is not None:\n",
    "        docs.append(web_results)\n",
    "    else:\n",
    "        docs = [web_results]\n",
    "    return {\"documents\": docs, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Condition edges\n",
    "\n",
    "def decide_to_generate(state):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Decide whether to generate answer or do web search\n",
    "    \n",
    "    input: state(dict):  graph state\n",
    "    output: bool: the next node to be executed.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"*** Assess Graded Documents ***\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == True:\n",
    "        print(\"Web search is required\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        print(\"Web search is not required\")\n",
    "        return \"generate\"\n",
    "    \n",
    "def is_hallucination_and_useful(state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Checks for hallucination\n",
    "    \n",
    "    input: state(dict):  graph state\n",
    "    output: bool: the next node to be executed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*** Assessing Hallucination ***\")\n",
    "    question = state[\"question\"]\n",
    "    answer = state[\"answer\"]\n",
    "    docs = state[\"documents\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": docs, \"answer\": answer})\n",
    "    grade = score[\"score\"]\n",
    "    if grade.lower() == \"yes\":\n",
    "        print(\"Graded: Answer is grounded\")\n",
    "\n",
    "        print(\"Grading answer vs question\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"answer\": answer})\n",
    "        grade = score[\"score\"]\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"Graded: Answer is useful\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"Graded: Answer is not useful\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"Graded: Answer is not grounded, Re-try\")\n",
    "        return \"not grounded/supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x14f163740>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x14f163740>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")  \n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    is_hallucination_and_useful,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\", \n",
    "        \"not grounded/supported\": \"generate\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\n",
    "    \"question\": \"Is TensorFlow created by Google?\",\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for k,v in output.items():\n",
    "        pprint(f\"Finished running: {k}:\")\n",
    "print(\"*****************************\",v[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
